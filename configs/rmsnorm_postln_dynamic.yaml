experiment:
  norm_type: "postln"    # "postln" or "rmsnorm"
  masking_type: "dynamic"  # "static" or "dynamic"

output:
  base_dir: "./result-rmsnorm"

model:
  name: "bert-large-uncased"  # or "bert-base-uncased", "prajjwal1/bert-small"

dataset:
  name: "wikitext"
  config: "wikitext-103-raw-v1"
  train_sample_size: null  # or 100000
  max_length: 64

training:
  batch_size: 8
  max_steps: 3000
  lr: 5.0e-5
  warmup_steps: 100
  eval_steps: 100
  logging_steps: 10
  seeds: [42, 43, 44]
